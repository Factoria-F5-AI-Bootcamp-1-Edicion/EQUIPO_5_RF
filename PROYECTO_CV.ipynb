{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a1fc11b-28cb-4a37-b29a-d9d89d98230d",
      "metadata": {
        "id": "0a1fc11b-28cb-4a37-b29a-d9d89d98230d"
      },
      "source": [
        "# RECONOCIMIENTO FACIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe81e0df-93b0-4d63-be3d-f84d3ee67cfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "fe81e0df-93b0-4d63-be3d-f84d3ee67cfc",
        "outputId": "f3d8105b-d1c7-494b-9673-1a7423047ef2"
      },
      "outputs": [],
      "source": [
        "# Lectura de imágenes\n",
        "# ==============================================================================\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imagen_1 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/Foto1.jpg')\n",
        "imagen_2 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/Foto_grupal.jpg')\n",
        "imagen_3 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/Foto_grupal_2.jpg')\n",
        "imagen_4 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/mayra.jpg')\n",
        "imagen_5 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/andrea.png')\n",
        "imagen_6 = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/reco_1.jpg')\n",
        "\n",
        "\n",
        "# Representación de imágenes\n",
        "# ==============================================================================\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(imagen_1)\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(imagen_2)\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(imagen_3)\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(imagen_4)\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(imagen_5)\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(imagen_6)\n",
        "plt.axis('off');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6749fdc-d3ec-47ac-aaa2-2dee66c3572e",
      "metadata": {
        "id": "a6749fdc-d3ec-47ac-aaa2-2dee66c3572e"
      },
      "source": [
        "## DETECTOR DE CARAS MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd73b66-e5fe-4f01-96ad-6b22a8791eaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ffd73b66-e5fe-4f01-96ad-6b22a8791eaa",
        "outputId": "a728a0b2-731f-4aff-b41f-3f5fd687f222"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from facenet_pytorch import MTCNN\n",
        "import numpy as np\n",
        "\n",
        "# Detectar si se dispone de GPU cuda\n",
        "# ==============================================================================\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))\n",
        "\n",
        "# Detector MTCNN\n",
        "# ==============================================================================\n",
        "mtcnn = MTCNN(\n",
        "            select_largest = True,\n",
        "            min_face_size  = 20,\n",
        "            thresholds     = [0.6, 0.7, 0.7],\n",
        "            post_process   = False,\n",
        "            image_size     = 160,\n",
        "            device         = device\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dda2dc2-2820-4c57-8834-9372afc0fd0c",
      "metadata": {
        "id": "1dda2dc2-2820-4c57-8834-9372afc0fd0c"
      },
      "outputs": [],
      "source": [
        "# Detección de bounding box y landmarks\n",
        "# ==============================================================================\n",
        "boxes, probs, landmarks = mtcnn.detect(imagen_4, landmarks=True)\n",
        "print('Bounding boxes:', boxes)\n",
        "print('Probability:', probs)\n",
        "print('landmarks:', landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0f4092-d0ec-4c25-ba2e-2577cb569922",
      "metadata": {
        "id": "3b0f4092-d0ec-4c25-ba2e-2577cb569922"
      },
      "outputs": [],
      "source": [
        "# Representación con matplotlib\n",
        "# ==============================================================================\n",
        "# En punto de origen (0,0) de una imagen es la esquina superior izquierda\n",
        "box = boxes[0]\n",
        "landmark = landmarks[0]\n",
        "fig, ax  = plt.subplots(figsize=(5, 4))\n",
        "ax.imshow(imagen_4)\n",
        "ax.scatter(landmark[:, 0], landmark[:, 1], s=8, c= 'red')\n",
        "rect = plt.Rectangle(\n",
        "            xy     = (box[0], box[1]),\n",
        "            width  = box[2] - box[0],\n",
        "            height = box[3] - box[1],\n",
        "            fill   = False,\n",
        "            color  = 'red'\n",
        "       )\n",
        "ax.add_patch(rect)\n",
        "ax.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f20361-fd88-478c-b308-a7d29c48c7cd",
      "metadata": {
        "id": "02f20361-fd88-478c-b308-a7d29c48c7cd"
      },
      "outputs": [],
      "source": [
        "# Detección de bounding box y landmarks\n",
        "# ==============================================================================\n",
        "boxes, probs, landmarks = mtcnn.detect(imagen_3, landmarks=True)\n",
        "\n",
        "# Representación con matplotlib\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "ax.imshow(imagen_3)\n",
        "\n",
        "for box, landmark in zip(boxes, landmarks):\n",
        "    ax.scatter(landmark[:, 0], landmark[:, 1], s=8, c= 'red')\n",
        "    rect = plt.Rectangle(\n",
        "                xy     = (box[0], box[1]),\n",
        "                width  = box[2] - box[0],\n",
        "                height = box[3] - box[1],\n",
        "                fill   = False,\n",
        "                color  = 'red'\n",
        "           )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "ax.axis('off');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0488aa63-982f-41d9-88cc-527d186ac9e0",
      "metadata": {
        "id": "0488aa63-982f-41d9-88cc-527d186ac9e0"
      },
      "outputs": [],
      "source": [
        "# Detección de cara\n",
        "# ==============================================================================\n",
        "# La imagen devuelta por el detector es un tensor con dimensiones [3, 160, 160]\n",
        "# [canales color, altura, anchura]\n",
        "# canales color ---> 3 ---> RGB\n",
        "face = mtcnn.forward(imagen_2)\n",
        "\n",
        "# Representación con matplotlib\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
        "face = face.permute(1, 2, 0).int().numpy()\n",
        "ax.imshow(face)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740cab0f-af8c-49cc-8df0-7d622892c52c",
      "metadata": {
        "id": "740cab0f-af8c-49cc-8df0-7d622892c52c"
      },
      "outputs": [],
      "source": [
        "# Detector MTCNN\n",
        "# ==============================================================================\n",
        "mtcnn = MTCNN(\n",
        "            keep_all      = True,\n",
        "            min_face_size = 20,\n",
        "            thresholds    = [0.6, 0.7, 0.7],\n",
        "            post_process  = False,\n",
        "            image_size    = 160,\n",
        "            device        = device\n",
        "        )\n",
        "\n",
        "# Detección de caras\n",
        "# ==============================================================================\n",
        "faces = mtcnn.forward(imagen_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b670119-24e2-4270-9d3d-68050ae3f960",
      "metadata": {
        "id": "7b670119-24e2-4270-9d3d-68050ae3f960"
      },
      "outputs": [],
      "source": [
        "# Las dimensiones del tensor generado son [nº caras, canales color, altura, anchura].\n",
        "faces.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8c182e-a65e-4453-ba53-c5fb61153dc5",
      "metadata": {
        "id": "7b8c182e-a65e-4453-ba53-c5fb61153dc5"
      },
      "outputs": [],
      "source": [
        "# Representación con matplotlib\n",
        "# ==============================================================================\n",
        "fig, axs = plt.subplots(nrows=2, ncols=6, figsize=(20, 6))\n",
        "axs= axs.flatten()\n",
        "\n",
        "for i in range(faces.shape[0]):\n",
        "    face = faces[i,:,:,:].permute(1, 2, 0).int().numpy()\n",
        "    axs[i].imshow(face)\n",
        "    axs[i].axis('off')\n",
        "    \n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "475672c2-d2ec-4c89-974c-d2691d5820dc",
      "metadata": {
        "id": "475672c2-d2ec-4c89-974c-d2691d5820dc"
      },
      "outputs": [],
      "source": [
        "# Extracción de cara a partir de una bounding box\n",
        "# ==============================================================================\n",
        "boxes, probs, landmarks = mtcnn.detect(imagen_4, landmarks=True)\n",
        "x1, y1, x2, y2 = boxes[0].astype(int)\n",
        "recorte_cara = np.array(imagen_4)[y1:y2, x1:x2]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
        "plt.imshow(recorte_cara)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6018e93f-2404-44e1-bc6c-b42fba0c855e",
      "metadata": {
        "id": "6018e93f-2404-44e1-bc6c-b42fba0c855e"
      },
      "source": [
        "## EMBEDDING DE ROSTROS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4911aa-fdb2-4644-894d-1c3a25d2e627",
      "metadata": {
        "id": "9d4911aa-fdb2-4644-894d-1c3a25d2e627"
      },
      "outputs": [],
      "source": [
        "# Modelo para hacer el embedding de las caras\n",
        "# ==============================================================================\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "encoder = InceptionResnetV1(pretrained='vggface2', classify=False, device=device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aad9f1f-b98b-424e-b562-3396e78441cf",
      "metadata": {
        "id": "9aad9f1f-b98b-424e-b562-3396e78441cf"
      },
      "outputs": [],
      "source": [
        "# Detección de cara\n",
        "# ==============================================================================\n",
        "cara = mtcnn(imagen_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942d6232-c0d4-4fc3-8899-d37fce8bcc46",
      "metadata": {
        "id": "942d6232-c0d4-4fc3-8899-d37fce8bcc46"
      },
      "outputs": [],
      "source": [
        "# Embedding de cara\n",
        "# ==============================================================================\n",
        "embedding_cara = encoder.forward(cara.reshape((1,3, 160, 160))).detach().cpu()\n",
        "embedding_cara"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb35e2f-5871-436f-985c-97c885ecb3c1",
      "metadata": {
        "id": "1eb35e2f-5871-436f-985c-97c885ecb3c1"
      },
      "source": [
        "## SIMILITUD ENTRE ROSTROS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ae80d8-07e3-44be-94ca-cab01cc90fa1",
      "metadata": {
        "id": "d0ae80d8-07e3-44be-94ca-cab01cc90fa1"
      },
      "outputs": [],
      "source": [
        "# Detector MTCNN\n",
        "# ==============================================================================\n",
        "mtcnn = MTCNN(\n",
        "            keep_all      = True,\n",
        "            min_face_size = 20,\n",
        "            thresholds    = [0.6, 0.7, 0.7],\n",
        "            post_process  = False,\n",
        "            image_size    = 160,\n",
        "            device        = device\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2643b1b6-b978-453f-b36a-dbb05afe1ab7",
      "metadata": {
        "id": "2643b1b6-b978-453f-b36a-dbb05afe1ab7"
      },
      "outputs": [],
      "source": [
        "# Extracción de las caras MTCNN\n",
        "# ==============================================================================\n",
        "PRUEBA_2 = mtcnn(imagen_4)[0]\n",
        "PRUEBA_1= mtcnn(imagen_1)[2]\n",
        "RECO = mtcnn(imagen_1)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf890a5a-7baf-4ad0-9765-ac0056a4b94d",
      "metadata": {
        "id": "cf890a5a-7baf-4ad0-9765-ac0056a4b94d"
      },
      "outputs": [],
      "source": [
        "# Representación con matplotlib\n",
        "# ==============================================================================\n",
        "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 6))\n",
        "\n",
        "face = PRUEBA_2.permute(1, 2, 0).int().numpy()\n",
        "axs[0].imshow(face)\n",
        "axs[0].set_title('MAYRA 1')\n",
        "axs[0].axis('off')\n",
        "\n",
        "face = PRUEBA_1.permute(1, 2, 0).int().numpy()\n",
        "axs[1].imshow(face)\n",
        "axs[1].set_title('MAYRA 2')\n",
        "axs[1].axis('off')\n",
        "\n",
        "face = RECO.permute(1, 2, 0).int().numpy()\n",
        "axs[2].imshow(face)\n",
        "axs[2].set_title('CHEN')\n",
        "axs[2].axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e818eba7-aac4-4558-ba3b-ae3586b62e0f",
      "metadata": {
        "id": "e818eba7-aac4-4558-ba3b-ae3586b62e0f"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "# ==============================================================================\n",
        "embeding_PRUEBA_2 = encoder.forward(PRUEBA_2.reshape((1,3, 160, 160))).detach().cpu()\n",
        "embeding_PRUEBA_1 = encoder.forward(PRUEBA_1.reshape((1,3, 160, 160))).detach().cpu()\n",
        "embeding_RECO = encoder.forward(RECO.reshape((1,3, 160, 160))).detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e587226d-203e-40bc-bb6d-a202f59ea1cf",
      "metadata": {
        "id": "e587226d-203e-40bc-bb6d-a202f59ea1cf"
      },
      "outputs": [],
      "source": [
        "embeding_PRUEBA_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534499bf-3a0b-4bb0-a91a-b73f50ac0930",
      "metadata": {
        "id": "534499bf-3a0b-4bb0-a91a-b73f50ac0930"
      },
      "outputs": [],
      "source": [
        "# Distancias entre embeddings de caras\n",
        "# ==============================================================================\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "print(f\"Distancia entre la misma imagen MAYRA: {euclidean_distances(embeding_PRUEBA_2, embeding_PRUEBA_2)}\")\n",
        "print(f\"Distancia entre las dos imágenes de MAYRA: {euclidean_distances(embeding_PRUEBA_2, embeding_PRUEBA_1)}\")\n",
        "print(f\"Distancia entre MAYRA y CHEN: {euclidean_distances(embeding_PRUEBA_2, embeding_RECO)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793b0c50-8050-42a3-9998-f683948eadc5",
      "metadata": {
        "id": "793b0c50-8050-42a3-9998-f683948eadc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Librerías\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import warnings\n",
        "import typing\n",
        "import logging\n",
        "import os\n",
        "import platform\n",
        "import glob\n",
        "import PIL\n",
        "import facenet_pytorch\n",
        "from typing import Union, Dict\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(\n",
        "    format = '%(asctime)-5s %(name)-10s %(levelname)-5s %(message)s', \n",
        "    level  = logging.WARNING,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "io_0beJJESq1",
      "metadata": {
        "id": "io_0beJJESq1"
      },
      "outputs": [],
      "source": [
        "# Funciones para la detección, extracción, embedding, identificación y gráficos\n",
        "# ==============================================================================\n",
        "def detectar_caras(imagen: Union[PIL.Image.Image, np.ndarray],\n",
        "                   detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
        "                   keep_all: bool        = True,\n",
        "                   min_face_size: int    = 20,\n",
        "                   thresholds: list      = [0.6, 0.7, 0.7],\n",
        "                   device: str           = None,\n",
        "                   min_confidence: float = 0.5,\n",
        "                   fix_bbox: bool        = True,\n",
        "                   verbose               = False)-> np.ndarray:\n",
        "                  \n",
        "\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
        "        raise Exception(\n",
        "            f\"`imagen` debe ser `np.ndarray, PIL.Image`. Recibido {type(imagen)}.\"\n",
        "        )\n",
        "\n",
        "    if detector is None:\n",
        "        logging.info('Iniciando detector MTCC')\n",
        "        detector = MTCNN(\n",
        "                        keep_all      = keep_all,\n",
        "                        min_face_size = min_face_size,\n",
        "                        thresholds    = thresholds,\n",
        "                        post_process  = False,\n",
        "                        device        = device\n",
        "                   )\n",
        "        \n",
        "    # Detección de caras\n",
        "    # --------------------------------------------------------------------------\n",
        "    if isinstance(imagen, PIL.Image.Image):\n",
        "        imagen = np.array(imagen).astype(np.float32)\n",
        "        \n",
        "    bboxes, probs = detector.detect(imagen, landmarks=False)\n",
        "    \n",
        "    if bboxes is None:\n",
        "        bboxes = np.array([])\n",
        "        probs  = np.array([])\n",
        "    else:\n",
        "        # Se descartan caras con una probabilidad estimada inferior a `min_confidence`.\n",
        "        bboxes = bboxes[probs > min_confidence]\n",
        "        probs  = probs[probs > min_confidence]\n",
        "        \n",
        "    logging.info(f'Número total de caras detectadas: {len(bboxes)}')\n",
        "    logging.info(f'Número final de caras seleccionadas: {len(bboxes)}')\n",
        "\n",
        "\n",
        "\n",
        "    # Corregir bounding boxes\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Si alguna de las esquinas de la bounding box está fuera de la imagen, se\n",
        "    # corrigen para que no sobrepase los márgenes.\n",
        "    if len(bboxes) > 0 and fix_bbox:       \n",
        "        for i, bbox in enumerate(bboxes):\n",
        "            if bbox[0] < 0:\n",
        "                bboxes[i][0] = 0\n",
        "            if bbox[1] < 0:\n",
        "                bboxes[i][1] = 0\n",
        "            if bbox[2] > imagen.shape[1]:\n",
        "                bboxes[i][2] = imagen.shape[1]\n",
        "            if bbox[3] > imagen.shape[0]:\n",
        "                bboxes[i][3] = imagen.shape[0]\n",
        "                \n",
        "\n",
        "    # Información de proceso\n",
        "    # ----------------------------------------------------------------------\n",
        "    if verbose:\n",
        "        print(\"----------------\")\n",
        "        print(\"Imagen escaneada\")\n",
        "        print(\"----------------\")\n",
        "        print(f\"Caras detectadas: {len(bboxes)}\")\n",
        "        print(f\"Correción bounding boxes: {fix_bbox}\")\n",
        "        print(f\"Coordenadas bounding boxes: {bboxes}\")\n",
        "        print(f\"Confianza bounding boxes:{probs} \")\n",
        "        print(\"\")\n",
        "        \n",
        "    return bboxes.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ulyEkLQCFhS1",
      "metadata": {
        "id": "ulyEkLQCFhS1"
      },
      "outputs": [],
      "source": [
        "def mostrar_bboxes(imagen: Union[PIL.Image.Image, np.ndarray],\n",
        "                   bboxes: np.ndarray,\n",
        "                   identidades: list=None,\n",
        "                   ax=None ) -> None:\n",
        "\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
        "        raise Exception(\n",
        "            f\"`imagen` debe ser `np.ndarray, PIL.Image`. Recibido {type(imagen)}.\"\n",
        "        )\n",
        "        \n",
        "    if identidades is not None:\n",
        "        if len(bboxes) != len(identidades):\n",
        "            raise Exception(\n",
        "                '`identidades` debe tener el mismo número de elementos que `bboxes`.'\n",
        "            )\n",
        "    else:\n",
        "        identidades = [None] * len(bboxes)\n",
        "\n",
        "    # Mostrar la imagen y superponer bounding boxes\n",
        "    # --------------------------------------------------------------------------\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "        \n",
        "    if isinstance(imagen, PIL.Image.Image):\n",
        "        imagen = np.array(imagen).astype(np.float32) / 255\n",
        "        \n",
        "    ax.imshow(imagen)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    if len(bboxes) > 0:\n",
        "        \n",
        "        for i, bbox in enumerate(bboxes):\n",
        "            if identidades[i] is not None:\n",
        "                rect = plt.Rectangle(\n",
        "                            xy        = (bbox[0], bbox[1]),\n",
        "                            width     = bbox[2] - bbox[0],\n",
        "                            height    = bbox[3] - bbox[1],\n",
        "                            linewidth = 1,\n",
        "                            edgecolor = 'lime',\n",
        "                            facecolor = 'none'\n",
        "                        )\n",
        "                \n",
        "                ax.add_patch(rect)\n",
        "                \n",
        "                ax.text(\n",
        "                    x = bbox[0],\n",
        "                    y = bbox[1] -10,\n",
        "                    s = identidades[i],\n",
        "                    fontsize = 10,\n",
        "                    color    = 'lime'\n",
        "                )\n",
        "            else:\n",
        "                rect = plt.Rectangle(\n",
        "                            xy        = (bbox[0], bbox[1]),\n",
        "                            width     = bbox[2] - bbox[0],\n",
        "                            height    = bbox[3] - bbox[1],\n",
        "                            linewidth = 1,\n",
        "                            edgecolor = 'red',\n",
        "                            facecolor = 'none'\n",
        "                        )\n",
        "                \n",
        "                ax.add_patch(rect)\n",
        "                \n",
        "        plt.show()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P1dksJ8eGd_R",
      "metadata": {
        "id": "P1dksJ8eGd_R"
      },
      "outputs": [],
      "source": [
        "def mostrar_bboxes_cv2(imagen: Union[PIL.Image.Image, np.ndarray],\n",
        "                       bboxes: np.ndarray,\n",
        "                       identidades: list=None,\n",
        "                       device: str='window') -> None:\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
        "        raise Exception(\n",
        "            f\"`imagen` debe ser `np.ndarray`, `PIL.Image`. Recibido {type(imagen)}.\"\n",
        "        )\n",
        "        \n",
        "    if identidades is not None:\n",
        "        if len(bboxes) != len(identidades):\n",
        "            raise Exception(\n",
        "                '`identidades` debe tener el mismo número de elementos que `bboxes`.'\n",
        "            )\n",
        "    else:\n",
        "        identidades = [None] * len(bboxes)\n",
        "\n",
        "    # Mostrar la imagen y superponer bounding boxes\n",
        "    # --------------------------------------------------------------------------      \n",
        "    if isinstance(imagen, PIL.Image.Image):\n",
        "        imagen = np.array(imagen).astype(np.float32) / 255\n",
        "    \n",
        "    if len(bboxes) > 0:\n",
        "        \n",
        "        for i, bbox in enumerate(bboxes):\n",
        "            \n",
        "            if identidades[i] is not None:\n",
        "                cv2.rectangle(\n",
        "                    img       = imagen,\n",
        "                    pt1       = (bbox[0], bbox[1]),\n",
        "                    pt2       = (bbox[2], bbox[3]),\n",
        "                    color     = (0, 255, 0),\n",
        "                    thickness = 2\n",
        "                )\n",
        "                \n",
        "                cv2.putText(\n",
        "                    img       = imagen, \n",
        "                    text      = identidades[i], \n",
        "                    org       = (bbox[0], bbox[1]-10), \n",
        "                    fontFace  = cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    fontScale = 1e-3 * imagen.shape[0],\n",
        "                    color     = (0,255,0),\n",
        "                    thickness = 2\n",
        "                )\n",
        "            else:\n",
        "                cv2.rectangle(\n",
        "                    img       = imagen,\n",
        "                    pt1       = (bbox[0], bbox[1]),\n",
        "                    pt2       = (bbox[2], bbox[3]),\n",
        "                    color     = (255, 0, 0),\n",
        "                    thickness = 2\n",
        "                )\n",
        "        \n",
        "    if device is None:\n",
        "        return imagen\n",
        "    else:\n",
        "        cv2.imshow(device, cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB))\n",
        "        if cv2.waitKey(1) == 27: \n",
        "            brcv2.destroyAllWindows()  # esc para cerrar la ventana                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2vcvm-eHHJel",
      "metadata": {
        "id": "2vcvm-eHHJel"
      },
      "outputs": [],
      "source": [
        "def extraer_caras(imagen: Union[PIL.Image.Image, np.ndarray],\n",
        "                  bboxes: np.ndarray,\n",
        "                  output_img_size: Union[list, tuple, np.ndarray]=[160, 160]) -> None:\n",
        "\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
        "        raise Exception(\n",
        "            f\"`imagen` debe ser np.ndarray, PIL.Image. Recibido {type(imagen)}.\"\n",
        "        )\n",
        "        \n",
        "    # Recorte de cara\n",
        "    # --------------------------------------------------------------------------\n",
        "    if isinstance(imagen, PIL.Image.Image):\n",
        "        imagen = np.array(imagen)\n",
        "        \n",
        "    if len(bboxes) > 0:\n",
        "        caras = []\n",
        "        for bbox in bboxes:\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            cara = imagen[y1:y2, x1:x2]\n",
        "            # Redimensionamiento del recorte\n",
        "            cara = Image.fromarray(cara)\n",
        "            cara = cara.resize(tuple(output_img_size))\n",
        "            cara = np.array(cara)\n",
        "            caras.append(cara)\n",
        "            \n",
        "    caras = np.stack(caras, axis=0)\n",
        "\n",
        "    return caras   \n",
        "\n",
        "\n",
        "\n",
        "def calcular_embeddings(img_caras: np.ndarray, encoder=None,\n",
        "                        device: str=None) -> np.ndarray:        \n",
        "\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not isinstance(img_caras, np.ndarray):\n",
        "        raise Exception(\n",
        "            f\"`img_caras` debe ser np.ndarray {type(img_caras)}.\"\n",
        "        )\n",
        "        \n",
        "    if img_caras.ndim != 4:\n",
        "        raise Exception(\n",
        "            f\"`img_caras` debe ser np.ndarray con dimensiones [nº caras, ancho, alto, 3].\"\n",
        "            f\" Recibido {img_caras.ndim}.\"\n",
        "        )\n",
        "        \n",
        "    if encoder is None:\n",
        "        logging.info('Iniciando encoder InceptionResnetV1')\n",
        "        encoder = InceptionResnetV1(\n",
        "                        pretrained = 'vggface2',\n",
        "                        classify   = False,\n",
        "                        device     = device\n",
        "                   ).eval()\n",
        "        \n",
        "    # Calculo de embedings\n",
        "    # --------------------------------------------------------------------------\n",
        "    # El InceptionResnetV1 modelo requiere que las dimensiones de entrada sean\n",
        "    # [nº caras, 3, ancho, alto]\n",
        "    caras = np.moveaxis(img_caras, -1,1)\n",
        "    caras = caras.astype(np.float32) / 255\n",
        "    caras = torch.tensor(caras)\n",
        "    embeddings = encoder.forward(caras).detach().cpu().numpy()\n",
        "    embeddings = embeddings\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def identificar_caras(embeddings: np.ndarray,\n",
        "                      dic_referencia: dict,\n",
        "                      threshold_similaridad: float = 0.6) -> list:\n",
        "    identidades = []\n",
        "        \n",
        "    for i in range(embeddings.shape[0]):\n",
        "        # Se calcula la similitud con cada uno de los perfiles de referencia.\n",
        "        similitudes = {}\n",
        "        for key, value in dic_referencia.items():\n",
        "            \n",
        "            similitudes[key] = 1 - cosine(embeddings[i], value[0])\n",
        "        \n",
        "        # Se identifica la persona de mayor similitud.\n",
        "        identidad = max(similitudes, key=similitudes.get)\n",
        "        # Si la similitud < threshold_similaridad, se etiqueta como None\n",
        "        if similitudes[identidad] < threshold_similaridad:\n",
        "            identidad = None\n",
        "            \n",
        "        identidades.append(identidad)\n",
        "        \n",
        "    return identidades\n",
        "\n",
        "\n",
        "\n",
        "def crear_diccionario_referencias(folder_path:str,\n",
        "                                  dic_referencia:dict=None,\n",
        "                                  detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
        "                                  min_face_size: int=40,\n",
        "                                  thresholds: list=[0.6, 0.7, 0.7],\n",
        "                                  min_confidence: float=0.9,\n",
        "                                  encoder=None,\n",
        "                                  device: str=None,\n",
        "                                  verbose: bool=False)-> dict:\n",
        "\n",
        "    # Comprobaciones iniciales\n",
        "    # --------------------------------------------------------------------------\n",
        "    if not os.path.isdir(folder_path):\n",
        "        raise Exception(\n",
        "            f\"Directorio {folder_path} no existe.\"\n",
        "        )\n",
        "        \n",
        "    if len(os.listdir(folder_path) ) == 0:\n",
        "        raise Exception(\n",
        "            f\"Directorio {folder_path} está vacío.\"\n",
        "        )\n",
        "    \n",
        "    \n",
        "    if detector is None:\n",
        "        logging.info('Iniciando detector MTCC')\n",
        "        detector = MTCNN(\n",
        "                        keep_all      = False,\n",
        "                        post_process  = False,\n",
        "                        min_face_size = min_face_size,\n",
        "                        thresholds    = thresholds,\n",
        "                        device        = device\n",
        "                   )\n",
        "    \n",
        "    if encoder is None:\n",
        "        logging.info('Iniciando encoder InceptionResnetV1')\n",
        "        encoder = InceptionResnetV1(\n",
        "                        pretrained = 'vggface2',\n",
        "                        classify   = False,\n",
        "                        device     = device\n",
        "                   ).eval()\n",
        "        \n",
        "    \n",
        "    new_dic_referencia = {}\n",
        "    folders = glob.glob(folder_path + \"/*\")\n",
        "    \n",
        "    for folder in folders:\n",
        "        \n",
        "        if platform.system() in ['Linux', 'Darwin']:\n",
        "            identidad = folder.split(\"/\")[-1]\n",
        "        else:\n",
        "            identidad = folder.split(\"\\\\\")[-1]\n",
        "                                     \n",
        "        logging.info(f'Obteniendo embeddings de: {identidad}')\n",
        "        embeddings = []\n",
        "        # Se lista todas las imagenes .jpg .jpeg .tif .png\n",
        "        path_imagenes = glob.glob(folder + \"/*.jpg\")\n",
        "        path_imagenes.extend(glob.glob(folder + \"/*.jpeg\"))\n",
        "        path_imagenes.extend(glob.glob(folder + \"/*.tif\"))\n",
        "        path_imagenes.extend(glob.glob(folder + \"/*.png\"))\n",
        "        logging.info(f'Total imagenes referencia: {len(path_imagenes)}')\n",
        "        \n",
        "        for path_imagen in path_imagenes:\n",
        "            logging.info(f'Leyendo imagen: {path_imagen}')\n",
        "            imagen = Image.open(path_imagen)\n",
        "            # Si la imagen es RGBA se pasa a RGB\n",
        "            if np.array(imagen).shape[2] == 4:\n",
        "                imagen  = np.array(imagen)[:, :, :3]\n",
        "                imagen  = Image.fromarray(imagen)\n",
        "                \n",
        "            bbox = detectar_caras(\n",
        "                        imagen,\n",
        "                        detector       = detector,\n",
        "                        min_confidence = min_confidence,\n",
        "                        verbose        = False\n",
        "                    )\n",
        "            \n",
        "            if len(bbox) > 1:\n",
        "                logging.warning(\n",
        "                    f'Más de 2 caras detectadas en la imagen: {path_imagen}. '\n",
        "                    f'Se descarta la imagen del diccionario de referencia.'\n",
        "                )\n",
        "                continue\n",
        "                \n",
        "            if len(bbox) == 0:\n",
        "                logging.warning(\n",
        "                    f'No se han detectado caras en la imagen: {path_imagen}.'\n",
        "                )\n",
        "                continue\n",
        "                \n",
        "            cara = extraer_caras(imagen, bbox)\n",
        "            embedding = calcular_embeddings(cara, encoder=encoder)\n",
        "            embeddings.append(embedding)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Identidad: {identidad} --- Imágenes referencia: {len(embeddings)}\")\n",
        "            \n",
        "        embedding_promedio = np.array(embeddings).mean(axis = 0)\n",
        "        new_dic_referencia[identidad] = embedding_promedio\n",
        "        \n",
        "    if dic_referencia is not None:\n",
        "        dic_referencia.update(new_dic_referencia)\n",
        "        return dic_referencia\n",
        "    else:\n",
        "        return new_dic_referencia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h4Yn2-u9HydA",
      "metadata": {
        "id": "h4Yn2-u9HydA"
      },
      "outputs": [],
      "source": [
        "def pipeline_deteccion_imagen(imagen: Union[PIL.Image.Image, np.ndarray],\n",
        "                              dic_referencia:dict,\n",
        "                              detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
        "                              keep_all: bool=True,\n",
        "                              min_face_size: int=20,\n",
        "                              thresholds: list=[0.6, 0.7, 0.7],\n",
        "                              device: str=None,\n",
        "                              min_confidence: float=0.5,\n",
        "                              fix_bbox: bool=True,\n",
        "                              output_img_size: Union[list, tuple, np.ndarray]=[160, 160],\n",
        "                              encoder=None,\n",
        "                              threshold_similaridad: float=0.5,\n",
        "                              ax=None,\n",
        "                              verbose=False)-> None:\n",
        "    \n",
        "    \n",
        "    bboxes = detectar_caras(\n",
        "                imagen         = imagen,\n",
        "                detector       = detector,\n",
        "                keep_all       = keep_all,\n",
        "                min_face_size  = min_face_size,\n",
        "                thresholds     = thresholds,\n",
        "                device         = device,\n",
        "                min_confidence = min_confidence,\n",
        "                fix_bbox       = fix_bbox\n",
        "              )\n",
        "    \n",
        "    if len(bboxes) == 0:\n",
        "        \n",
        "        logging.info('No se han detectado caras en la imagen.')\n",
        "        mostrar_bboxes(\n",
        "            imagen      = imagen,\n",
        "            bboxes      = bboxes,\n",
        "            ax          = ax\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "    \n",
        "        caras = extraer_caras(\n",
        "                    imagen = imagen,\n",
        "                    bboxes = bboxes\n",
        "                )\n",
        "\n",
        "        embeddings = calcular_embeddings(\n",
        "                        img_caras = caras,\n",
        "                        encoder   = encoder\n",
        "                     )\n",
        "\n",
        "        identidades = identificar_caras(\n",
        "                         embeddings     = embeddings,\n",
        "                         dic_referencia = dic_referencia,\n",
        "                         threshold_similaridad = threshold_similaridad\n",
        "                       )\n",
        "\n",
        "        mostrar_bboxes(\n",
        "            imagen      = imagen,\n",
        "            bboxes      = bboxes,\n",
        "            identidades = identidades,\n",
        "            ax          = ax\n",
        "        )                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9187a8ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pipeline_deteccion_webcam(dic_referencia: dict,\n",
        "                             output_device: str = 'window',\n",
        "                             path_output_video: str=os.getcwd(),\n",
        "                             detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
        "                             keep_all: bool=True,\n",
        "                             min_face_size: int=40,\n",
        "                             thresholds: list=[0.6, 0.7, 0.7],\n",
        "                             device: str=None,\n",
        "                             min_confidence: float=0.5,\n",
        "                             fix_bbox: bool=True,\n",
        "                             output_img_size: Union[list, tuple, np.ndarray]=[160, 160],\n",
        "                             encoder=None,\n",
        "                             threshold_similaridad: float=0.5,\n",
        "                             ax=None,\n",
        "                             verbose=False)-> None:\n",
        "    \n",
        "\n",
        "    capture = cv2.VideoCapture(0)\n",
        "    frame_exist = True\n",
        "\n",
        "    while(frame_exist):\n",
        "        frame_exist, frame = capture.read()\n",
        "\n",
        "        if not frame_exist:\n",
        "            capture.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        bboxes = detectar_caras(\n",
        "                        imagen         = frame,\n",
        "                        detector       = detector,\n",
        "                        keep_all       = keep_all,\n",
        "                        min_face_size  = min_face_size,\n",
        "                        thresholds     = thresholds,\n",
        "                        device         = device,\n",
        "                        min_confidence = min_confidence,\n",
        "                        fix_bbox       = fix_bbox\n",
        "                      )\n",
        "\n",
        "        if len(bboxes) == 0:\n",
        "\n",
        "            logging.info('No se han detectado caras en la imagen.')\n",
        "            cv2.imshow(output_device, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                             \n",
        "        else:\n",
        "\n",
        "            caras = extraer_caras(\n",
        "                        imagen = frame,\n",
        "                        bboxes = bboxes\n",
        "                    )\n",
        "\n",
        "            embeddings = calcular_embeddings(\n",
        "                            img_caras = caras,\n",
        "                            encoder   = encoder\n",
        "                         )\n",
        "\n",
        "            identidades = identificar_caras(\n",
        "                             embeddings     = embeddings,\n",
        "                             dic_referencia = dic_referencia,\n",
        "                             threshold_similaridad = threshold_similaridad\n",
        "                          )\n",
        "\n",
        "            frame_procesado = mostrar_bboxes_cv2(\n",
        "                                imagen      = frame,\n",
        "                                bboxes      = bboxes,\n",
        "                                identidades = identidades,\n",
        "                                device = output_device\n",
        "                             )\n",
        "            \n",
        "        if cv2.waitKey(1) == 27: \n",
        "            break  # esc para cerrar la ventana\n",
        "\n",
        "    capture.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3447039a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#ruta: sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images\n",
        "\n",
        "# Detectar si se dispone de GPU cuda\n",
        "# ==============================================================================\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(F'Running on device: {device}')\n",
        "\n",
        "# Crear diccionario de referencia para cada persona\n",
        "# ==============================================================================\n",
        "dic_referencia = crear_diccionario_referencias(\n",
        "                    folder_path    = '/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/referencia',\n",
        "                    min_face_size  = 40,\n",
        "                    min_confidence = 0.9,\n",
        "                    device         = device,\n",
        "                    verbose        = True\n",
        "                  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5cf948",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconocimiento en imágenes\n",
        "# ==============================================================================\n",
        "# Detectar si se dispone de GPU cuda\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(F'Running on device: {device}')\n",
        "\n",
        "# Identificar las personas en la imagen\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "imagen = Image.open('/sysroot/home/andreasandoval/Documentos/BOOTCAMP_F5/RecoFacial/EQUIPO_5_RF/Images/Foto_grupal_2.jpg')\n",
        "\n",
        "pipeline_deteccion_imagen(\n",
        "    imagen = imagen,\n",
        "    dic_referencia        = dic_referencia,\n",
        "    min_face_size         = 20,\n",
        "    thresholds            = [0.6, 0.7, 0.7],\n",
        "    min_confidence        = 0.5,\n",
        "    threshold_similaridad = 0.4,\n",
        "    device                = device,\n",
        "    ax                    = ax,\n",
        "    verbose               = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9abba8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconocimiento por webcam\n",
        "# ==============================================================================\n",
        "pipeline_deteccion_webcam(\n",
        "     dic_referencia        = dic_referencia,\n",
        "     threshold_similaridad = 0.4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e09413",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ec8ef5af344d9b7cc2a9209b90a839f03f689773bf1d633cc88d40fd05eab2c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
